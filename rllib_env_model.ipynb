{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A deterministic (reproducible) example of a custom subclass Keras model with a simple custom Gymnasium environment\n",
        "Issues:\n",
        "1. Config with 'tf2' mode and eager tracing enabled reports\n",
        "   `input_dict[\"is_training\"]) KeyError: 'is_training'` \n",
        "2. 'tf' mode cannot restore policy from a checkpoint\n",
        "3. in 'tf' mode policy.export_model always saves a model with initial weights (not trained)\n",
        "\n",
        "Derived from https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/shmyak-ai/rllib-env-model/blob/main/rllib_env_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrYjvS7kJkqX"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  %pip install 'ray[default,rllib]' &>/dev/null || echo \"Ray install failed!\"\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9IoJy_NbKX1"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ray\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from ray.rllib.env.env_context import EnvContext\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.algorithms.algorithm_config import NotProvided\n",
        "from ray.tune.registry import get_trainable_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4XTVeWFelfL"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Arguments:\n",
        "    run: str = 'PPO'\n",
        "    framework: str = 'tf'\n",
        "    stop_iters: int = 10\n",
        "    stop_timesteps: int = 100000\n",
        "    stop_reward: float = 0.1 \n",
        "    local_mode: bool = False\n",
        "    num_workers: int = 0\n",
        "    num_envs_per_worker: int = 1\n",
        "    seed: int = 42\n",
        "\n",
        "args = Arguments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j7FVb4aAGE9"
      },
      "outputs": [],
      "source": [
        "if type(args.seed) is int:\n",
        "    tf.keras.utils.set_random_seed(args.seed)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    print(\"Tensorflow determenism is enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AyVTgance25"
      },
      "outputs": [],
      "source": [
        "class SimpleCorridor(gym.Env):\n",
        "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
        "    You can configure the length of the corridor via the env config.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EnvContext):\n",
        "        self.end_pos = config[\"corridor_length\"]\n",
        "        self.cur_pos = 0\n",
        "        self.action_space = Discrete(2)\n",
        "        self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
        "        # Set the seed. This is only used for the final (reach goal) reward.\n",
        "        if isinstance(config, EnvContext):\n",
        "            self.reset(seed=config[\"seed\"] + config.worker_index + config.num_workers)\n",
        "        else:\n",
        "            self.reset(seed=config[\"seed\"])\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        # random.seed(seed)\n",
        "        self.cur_pos = 0\n",
        "        return [self.cur_pos], {}\n",
        "\n",
        "    def step(self, action):\n",
        "        assert action in [0, 1], action\n",
        "        if action == 0 and self.cur_pos > 0:\n",
        "            self.cur_pos -= 1\n",
        "        elif action == 1:\n",
        "            self.cur_pos += 1\n",
        "        done = truncated = self.cur_pos >= self.end_pos\n",
        "        # Produce a random reward when we reach the goal.\n",
        "        return (\n",
        "            [self.cur_pos],\n",
        "            # random.random() * 2 if done else -0.1,\n",
        "            1 if done else -0.1,\n",
        "            done,\n",
        "            truncated,\n",
        "            {},\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8tBongjHKTj"
      },
      "outputs": [],
      "source": [
        "is_gpu = bool(tf.config.list_physical_devices('GPU'))\n",
        "num_gpus = NotProvided\n",
        "if is_gpu:\n",
        "    print(\"Use GPU\")\n",
        "    num_gpus = 1\n",
        "    # one more cpu for a driver\n",
        "    ray.init(local_mode=args.local_mode, num_cpus=args.num_workers + 1, num_gpus=num_gpus)\n",
        "else:\n",
        "    print(\"Use CPU\")\n",
        "    ray.init(local_mode=args.local_mode, num_cpus=args.num_workers + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZkjugkHhewG"
      },
      "outputs": [],
      "source": [
        "config = (\n",
        "    get_trainable_cls(args.run)\n",
        "    .get_default_config()\n",
        "    # or \"corridor\" if registered above\n",
        "    .environment(\n",
        "        SimpleCorridor, \n",
        "        env_config={\n",
        "            \"corridor_length\": 5,\n",
        "            \"seed\": args.seed,\n",
        "            },\n",
        "        )\n",
        "    .framework(\n",
        "        framework=args.framework,\n",
        "        eager_tracing=True if args.framework == 'tf2' else False,\n",
        "    )\n",
        "    .rollouts(\n",
        "        num_rollout_workers=args.num_workers,  # if 0 a driver will sample\n",
        "        num_envs_per_worker=args.num_envs_per_worker,  # 1 is minimum\n",
        "        )\n",
        "    .training(\n",
        "        model={\n",
        "            \"custom_model\": \"my_model\",\n",
        "            \"custom_model_config\": {\n",
        "                \"seed\": args.seed,\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "    .debugging(seed=args.seed)\n",
        "    .resources(num_gpus=num_gpus)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVUrRXWmQZ7E"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    A keras dense block.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_layers, num_outputs, seed, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        initializer = keras.initializers.VarianceScaling\n",
        "        self._dense = [layers.Dense(\n",
        "            n_features,\n",
        "            activation=tf.nn.silu,\n",
        "            kernel_initializer=initializer(\n",
        "                scale=2.0, \n",
        "                mode='fan_in', \n",
        "                distribution='truncated_normal', \n",
        "                seed=seed+i if type(seed) is int else None\n",
        "            )\n",
        "        ) for i in range(n_layers)]\n",
        "        self._out = layers.Dense(\n",
        "            num_outputs,\n",
        "            activation=None,\n",
        "            kernel_initializer=initializer(\n",
        "                scale=2.0, \n",
        "                mode='fan_in', \n",
        "                distribution='truncated_normal', \n",
        "                seed=seed+n_layers if type(seed) is int else None\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def call(self, input_tensor, *args, **kwargs):\n",
        "        x = input_tensor\n",
        "        for dense in self._dense:\n",
        "            x = dense(x)\n",
        "        return self._out(x)\n",
        "\n",
        "\n",
        "class DenseNet(keras.Model):\n",
        "    \"\"\"\n",
        "    A keras dense net.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_outputs, seed=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self._n_features = 256\n",
        "        self._n_layers = 2\n",
        "        self._actor = DenseBlock(self._n_features, self._n_layers, num_outputs, seed)\n",
        "        self._critic = DenseBlock(self._n_features, self._n_layers, 1, seed)\n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        logits = self._actor(input_tensor, training)\n",
        "        value = self._critic(input_tensor, training)\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "class CustomModel(TFModelV2):\n",
        "    \"\"\"Example of a keras custom model that just delegates to an fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        super(CustomModel, self).__init__(\n",
        "            obs_space, action_space, num_outputs, model_config, name\n",
        "        )\n",
        "        self.base_model = DenseNet(num_outputs, seed=model_config['custom_model_config']['seed'])\n",
        "        \n",
        "        fake_obs = tf.random.uniform(shape=obs_space.shape)\n",
        "        fake_obs = tf.expand_dims(fake_obs, 0)\n",
        "        _, self._value = self.base_model(fake_obs, False)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        logits, self._value = self.base_model(input_dict[\"obs\"])\n",
        "        # logits, self._value = self.base_model(input_dict[\"obs\"], input_dict[\"is_training\"])\n",
        "        return logits, state\n",
        "\n",
        "    def value_function(self):\n",
        "        return tf.reshape(self._value, [-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgMNiWP8hnS0"
      },
      "outputs": [],
      "source": [
        "ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n",
        "stop = {\n",
        "    \"training_iteration\": args.stop_iters,\n",
        "    \"timesteps_total\": args.stop_timesteps,\n",
        "    \"episode_reward_mean\": args.stop_reward,\n",
        "}\n",
        "\n",
        "# manual training with train loop using PPO and fixed learning rate\n",
        "if args.run != \"PPO\":\n",
        "    raise ValueError(\"Only support --run PPO with --no-tune.\")\n",
        "print(\"Running manual train loop without Ray Tune.\")\n",
        "# use fixed learning rate instead of grid search (needs tune)\n",
        "config.lr = 1e-3\n",
        "algo = config.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ7AKDSJT2Ec"
      },
      "outputs": [],
      "source": [
        "# run manual training loop and print results after each iteration\n",
        "checkpoint_paths = []\n",
        "for _ in range(args.stop_iters):\n",
        "    result = algo.train()\n",
        "    print(f\"Timesteps total: {result['agent_timesteps_total']}\")\n",
        "    print(f\"Episode reward: {result['episode_reward_mean']}\")\n",
        "    path_to_checkpoint = algo.save()\n",
        "    checkpoint_paths.append(path_to_checkpoint)\n",
        "    print(\n",
        "        \"An Algorithm checkpoint has been created inside directory: \"\n",
        "        f\"'{path_to_checkpoint}'.\"\n",
        "    )\n",
        "    policy = algo.get_policy()\n",
        "    policy.export_model(path_to_checkpoint + \"/keras_model\")\n",
        "    # stop training of the target train steps or reward are reached\n",
        "    if (\n",
        "        result[\"timesteps_total\"] >= args.stop_timesteps\n",
        "        or result[\"episode_reward_mean\"] >= args.stop_reward\n",
        "    ):\n",
        "        break\n",
        "algo.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoaurvGgJK0V"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Try to restore a policy or a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_to_checkpoint_1, path_to_checkpoint_2 = checkpoint_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAw4Gby_DYrH"
      },
      "outputs": [],
      "source": [
        "# ModelCatalog.register_custom_model(\"my_model\", CustomModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aMcDQTRYUeW"
      },
      "outputs": [],
      "source": [
        "# rllib_algorithm = Algorithm.from_checkpoint(path_to_checkpoint)\n",
        "# rllib_policy = rllib_algorithm.get_policy(\"default_policy\")\n",
        "# del rllib_algorithm\n",
        "# rllib_policy = Policy.from_checkpoint(path_to_checkpoint + \"/policies/default_policy\")\n",
        "keras_model_1 = tf.saved_model.load(path_to_checkpoint_1 + \"/keras_model/\")\n",
        "keras_model_2 = tf.saved_model.load(path_to_checkpoint_2 + \"/keras_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# policy_weights = rllib_policy.get_weights()\n",
        "# for key in policy_weights.keys():\n",
        "#     print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# policy_weights['default_policy/dense_net/dense_block/dense/kernel'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for var in keras_model.trainable_variables:\n",
        "#     print(var.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keras_model.trainable_variables[6].numpy().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# policy_weights['default_policy/dense_net/dense_block/dense/kernel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_model_1.trainable_variables[0].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_model_2.trainable_variables[0].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do4a5m3EEOEc"
      },
      "outputs": [],
      "source": [
        "# env = SimpleCorridor(config.env_config)\n",
        "# obs, _ = env.reset()\n",
        "# rllib_policy.compute_single_action(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXLyRQeoKAw5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "crypto",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d2bae772ed515d70b30a46d4bccfbbf0b6dbc543481ecffeaea4b6c2fdd20c0b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
